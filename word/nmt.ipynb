{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Michael\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Michael\\anaconda3\\Lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\Michael\\anaconda3\\Lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.12.0 and strictly below 2.15.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.15.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from tensorflow.python.layers import core as layers_core\n",
    "import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tfc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3100000it [00:04, 840862.55it/s]"
     ]
    }
   ],
   "source": [
    "enline, chline = None, None\n",
    "enlines, chlines = [], []\n",
    "sentence_length = 10\n",
    "file_path = '../data/word/segmented_train_seg_by_word.txt'\n",
    "pbar = tqdm()\n",
    "i = 0\n",
    "early_exist_line_num = 1000000\n",
    "with open(file_path, 'r') as lines:\n",
    "    for line in lines:\n",
    "        i += 1\n",
    "        if i % 2 == 1:\n",
    "            enline = line.lower().strip('\\n').split()\n",
    "            continue          \n",
    "        else:\n",
    "            chline =[w for w in line.strip('\\n').replace(' ','')]\n",
    "        if len(enline) > sentence_length and len(chline) > sentence_length:\n",
    "            continue    \n",
    "        enlines.append(enline)\n",
    "        chlines.append(chline)  \n",
    "        if i % 100000 == 0:\n",
    "            pbar.update(i)  \n",
    "        if i == early_exist_line_num:\n",
    "            break\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "301514"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['一', '对', '二', '总', '不', '是', '好', '事', '，'], ['一', '对', '二', '胜', '。'], ['一', '对', '五', '年', '没', '见', '过', '的', '姐', '妹', '一', '场', '激', '烈', '的', '争', '吵', '？'], ['一', '对', '五', '百', '诶', '。']]\n",
      "[['fighting', 'two', 'against', 'one', 'is', 'never', 'ideal', ','], ['deuces', 'the', 'winner', '.'], ['an', 'incredibly', 'emotional', 'fight', 'between', '2', 'sisters', '？'], ['one', 'against', '500', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(chlines[:4])\n",
    "print(enlines[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addchar(word2id, id2word, word):\n",
    "    id = len(word2id)\n",
    "    word2id[word] = id\n",
    "    id2word[id] = word\n",
    "\n",
    "ch2id, id2ch, en2id, id2en = {}, {}, {},{}\n",
    "most_common_k = 100000\n",
    "specialchars = ['<eos>','<start>','<end>','<unk>','<pad>']\n",
    "\n",
    "chwords, enwords = set(), set()\n",
    "for ch in chlines: \n",
    "    for c in ch:\n",
    "        chwords.add(c)\n",
    "for en in enlines: \n",
    "    for e in en:\n",
    "        enwords.add(e)\n",
    "\n",
    "for word, _ in Counter(chwords).most_common(most_common_k):\n",
    "    addchar(ch2id, id2ch, word)\n",
    "    \n",
    "for word, _ in Counter(enwords).most_common(most_common_k):\n",
    "    addchar(en2id, id2en, word)\n",
    "    \n",
    "for one in specialchars:\n",
    "    addchar(ch2id,id2ch,one)\n",
    "    addchar(en2id,id2en,one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47029, 4771)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en2id), len(ch2id)\n",
    "# ch2id['<unk>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for english to chinese translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_x_in, dat_y_in,dat_y_out =[], [], []\n",
    "dat_x_len, dat_y_len = [], []\n",
    "unknown_en_id = en2id['<unk>']\n",
    "eos_en_id = en2id['<eos>']\n",
    "# english sentences to id array\n",
    "for en_sentence in enlines:\n",
    "    id_en_sentence = [ en2id.get(w, unknown_en_id) for w in en_sentence ]\n",
    "    id_en_sentence.append(eos_en_id) #mark end of sentence\n",
    "    dat_x_in.append(id_en_sentence)\n",
    "    dat_x_len.append(len(id_en_sentence))\n",
    "    \n",
    "# chinse sentences to id array\n",
    "start_ch_id = ch2id['<start>']\n",
    "end_ch_id = ch2id['<end>']\n",
    "unknown_ch_id = ch2id['<unk>']\n",
    "for ch_sentence in chlines:\n",
    "    id_ch_sentence = [ ch2id.get(w, unknown_ch_id) for w in ch_sentence ]\n",
    "    dat_y_in.append([start_ch_id] + id_ch_sentence)\n",
    "    dat_y_out.append(id_ch_sentence + [end_ch_id])\n",
    "    dat_y_len.append(len(id_ch_sentence) + 1)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['one', 'pair', 'of', 'elongated', 'canines', '.', '<eos>'],\n",
       " ['<start>', '一', '对', '延', '长', '犬', '齿', '。'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = 100\n",
    "[id2en[i] for i in dat_x_in[m]], [id2ch[i] for i in dat_y_in[m]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3100000it [00:20, 840862.55it/s]"
     ]
    }
   ],
   "source": [
    "# build input and lable\n",
    "data_x_in, data_y = [],[]\n",
    "\n",
    "for ch_sentence, en_sentence in zip(chlines, enlines):\n",
    "    ch_id_sen = [ch2id.get(i, unknown_ch_id) for i in ch_sentence]\n",
    "    data_x_in.append(ch_id_sen)\n",
    "    data_y.append(0)\n",
    "\n",
    "    en_id_sen = [en2id.get(i, unknown_en_id) for i in en_sentence]\n",
    "    data_x_in.append(en_id_sen)\n",
    "    data_y.append(1)\n",
    "\n",
    "data_x_in = tf.keras.preprocessing.sequence.pad_sequences(data_x_in, padding='post', value=en2id['<pad>'])\n",
    "data_y = np.asarray(data_y)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buil model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "embedding_size = 100\n",
    "vocabulary_size = len(en2id)\n",
    "num_units = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 512   #word embedding vector length\n",
    "num_units = 512        #hidden layer size\n",
    "batch_size = 128\n",
    "max_grad = 1.0         #L2裁剪的c\n",
    "dropout = 0.2\n",
    "src_vocab_size = len(en2id) #vocabulary\n",
    "target_vocat_size = len(ch2id)\n",
    "seq_max_len = sentence_length + 1\n",
    "maximum_iterations = 10  #预测长度超过10时终结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping: no known devices.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michael\\AppData\\Local\\Temp\\ipykernel_16076\\2504026095.py:26: UserWarning: `tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  encoder_cell = tfc.nn.rnn_cell.BasicLSTMCell(num_units)\n",
      "C:\\Users\\Michael\\AppData\\Local\\Temp\\ipykernel_16076\\2504026095.py:35: UserWarning: `tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  decoder_cell = tfc.nn.rnn_cell.BasicLSTMCell(num_units)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "type of argument \"output_layer\" must be one of (keras.src.engine.base_layer.Layer, NoneType); got tensorflow.python.keras.legacy_tf_layers.core.Dense instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m     sampler \u001b[38;5;241m=\u001b[39m tfa\u001b[38;5;241m.\u001b[39mseq2seq\u001b[38;5;241m.\u001b[39mTrainingSampler(time_major\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     38\u001b[0m     sampler\u001b[38;5;241m.\u001b[39minitialize(decoder_embedding_inp, y_len)\n\u001b[1;32m---> 40\u001b[0m     decoder \u001b[38;5;241m=\u001b[39m tfa\u001b[38;5;241m.\u001b[39mseq2seq\u001b[38;5;241m.\u001b[39mBasicDecoder(decoder_cell, sampler, output_layer\u001b[38;5;241m=\u001b[39m projection_layer)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# loss\u001b[39;00m\n\u001b[0;32m     43\u001b[0m model_logistic \u001b[38;5;241m=\u001b[39m tfc\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mdense(encoder_states[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Michael\\anaconda3\\Lib\\site-packages\\typeguard\\__init__.py:1032\u001b[0m, in \u001b[0;36mtypechecked.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1030\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1031\u001b[0m     memo \u001b[38;5;241m=\u001b[39m _CallMemo(python_func, _localns, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m-> 1032\u001b[0m     check_argument_types(memo)\n\u001b[0;32m   1033\u001b[0m     retval \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1034\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Michael\\anaconda3\\Lib\\site-packages\\typeguard\\__init__.py:875\u001b[0m, in \u001b[0;36mcheck_argument_types\u001b[1;34m(memo)\u001b[0m\n\u001b[0;32m    873\u001b[0m             check_type(description, value, expected_type, memo)\n\u001b[0;32m    874\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:  \u001b[38;5;66;03m# suppress unnecessarily long tracebacks\u001b[39;00m\n\u001b[1;32m--> 875\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;241m*\u001b[39mexc\u001b[38;5;241m.\u001b[39margs) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: type of argument \"output_layer\" must be one of (keras.src.engine.base_layer.Layer, NoneType); got tensorflow.python.keras.legacy_tf_layers.core.Dense instead"
     ]
    }
   ],
   "source": [
    "tfc.reset_default_graph()\n",
    "tfc.disable_eager_execution()\n",
    "config = tfc.ConfigProto(log_device_placement =True, allow_soft_placement = True)\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tfc.Session(config = config)\n",
    "\n",
    "with tfc.device('/gpu:1'):\n",
    "    initializer =tfc.random_uniform_initializer(-0.08, 0.08)\n",
    "    tfc.get_variable_scope().set_initializer(initializer)\n",
    "\n",
    "    x = tfc.placeholder('int32', [None, None])\n",
    "    y = tfc.placeholder('int32', [None])\n",
    "    x_len = tfc.placeholder('int32', [None]) # control sequence lenth for RNN\n",
    "    y_len = tfc.placeholder('int32', [None]) # control sequence lenth for RNN\n",
    "    learning_rate = tfc.placeholder(tfc.float32, shape=[])\n",
    "\n",
    "    # embedding\n",
    "    embedding_encoder = tfc.get_variable('embedding_encoder', [vocabulary_size, embedding_size], dtype=tfc.float32)\n",
    "    embedding_decoder = tfc.get_variable('embedding_decoder',[target_vocat_size, embedding_size], dtype=tfc.float32)\n",
    "    encoder_embedding_inp = tfc.nn.embedding_lookup(embedding_encoder, x)\n",
    "    decoder_embedding_inp = tfc.nn.embedding_lookup(embedding_decoder, x)\n",
    "\n",
    "# encoder\n",
    "\n",
    "    # build rnn cell\n",
    "    encoder_cell = tfc.nn.rnn_cell.BasicLSTMCell(num_units)\n",
    "    encoder_outputs, encoder_states = tfc.nn.dynamic_rnn(encoder_cell, decoder_embedding_inp, sequence_length= x_len, time_major=False, dtype=tfc.float32)\n",
    "\n",
    "# decoder\n",
    "    batch_size_in = tf.shape(x)[0]\n",
    "    projection_layer = layers_core.Dense(len(ch2id), use_bias=False)\n",
    "    \n",
    "    # dynamic decoding\n",
    "    with tfc.variable_scope(\"decode_layer\"):\n",
    "        decoder_cell = tfc.nn.rnn_cell.BasicLSTMCell(num_units)\n",
    "        \n",
    "        sampler = tfa.seq2seq.TrainingSampler(time_major=False)\n",
    "        sampler.initialize(decoder_embedding_inp, y_len)\n",
    "        \n",
    "        decoder = tfa.seq2seq.BasicDecoder(decoder_cell, sampler, output_layer= projection_layer)\n",
    "\n",
    "    # loss\n",
    "    model_logistic = tfc.layers.dense(encoder_states[0], 1)\n",
    "    model_prediction = tfc.nn.sigmoid(model_logistic)\n",
    "    loss = tfc.nn.sigmoid_cross_entropy_with_logits(labels=tfc.cast(y, tfc.float32), logits=tfc.reshape(model_logistic, (-1,)))\n",
    "    loss1 = tfc.reduce_mean(loss)\n",
    "    optimizer = tfc.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generate(data_x_in, data_y, batch_size = 1000):\n",
    "    for i in range(0, len(data_x_in), batch_size):\n",
    "        if i + batch_size < len(data_x_in):\n",
    "            yield data_x_in[i: i + batch_size], data_y[i: i + batch_size]\n",
    "        else:\n",
    "            yield data_x_in[i: len(data_x_in)], data_y[i: len(data_x_in)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = []\n",
    "begging_learning_rate = 0.1\n",
    "gen = data_generate(data_x_in, data_y, batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
