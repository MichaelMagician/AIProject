{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from tensorflow.python.layers import core as layers_core\n",
    "import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tfc\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3100000it [00:02, 1785037.29it/s]"
     ]
    }
   ],
   "source": [
    "enline, chline = None, None\n",
    "enlines, chlines = [], []\n",
    "sentence_length = 10\n",
    "file_path = '../data/word/segmented_train_seg_by_word.txt'\n",
    "pbar = tqdm()\n",
    "i = 0\n",
    "early_exist_line_num = 1000000\n",
    "with open(file_path, 'r') as lines:\n",
    "    for line in lines:\n",
    "        i += 1\n",
    "        if i % 2 == 1:\n",
    "            enline = line.lower().strip('\\n').split()\n",
    "            continue          \n",
    "        else:\n",
    "            chline =[w for w in line.strip('\\n').replace(' ','')]\n",
    "        if len(enline) > sentence_length and len(chline) > sentence_length:\n",
    "            continue    \n",
    "        enlines.append(enline)\n",
    "        chlines.append(chline)  \n",
    "        if i % 100000 == 0:\n",
    "            pbar.update(i)  \n",
    "        if i == early_exist_line_num:\n",
    "            break\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "301514"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['一', '对', '二', '总', '不', '是', '好', '事', '，'], ['一', '对', '二', '胜', '。'], ['一', '对', '五', '年', '没', '见', '过', '的', '姐', '妹', '一', '场', '激', '烈', '的', '争', '吵', '？'], ['一', '对', '五', '百', '诶', '。']]\n",
      "[['fighting', 'two', 'against', 'one', 'is', 'never', 'ideal', ','], ['deuces', 'the', 'winner', '.'], ['an', 'incredibly', 'emotional', 'fight', 'between', '2', 'sisters', '？'], ['one', 'against', '500', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(chlines[:4])\n",
    "print(enlines[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addchar(word2id, id2word, word):\n",
    "    id = len(word2id)\n",
    "    word2id[word] = id\n",
    "    id2word[id] = word\n",
    "\n",
    "ch2id, id2ch, en2id, id2en = {}, {}, {},{}\n",
    "most_common_k = 100000\n",
    "specialchars = ['<eos>','<start>','<end>','<unk>','<pad>']\n",
    "\n",
    "chwords, enwords = set(), set()\n",
    "for ch in chlines: \n",
    "    for c in ch:\n",
    "        chwords.add(c)\n",
    "for en in enlines: \n",
    "    for e in en:\n",
    "        enwords.add(e)\n",
    "\n",
    "for word, _ in Counter(chwords).most_common(most_common_k):\n",
    "    addchar(ch2id, id2ch, word)\n",
    "    \n",
    "for word, _ in Counter(enwords).most_common(most_common_k):\n",
    "    addchar(en2id, id2en, word)\n",
    "    \n",
    "for one in specialchars:\n",
    "    addchar(ch2id,id2ch,one)\n",
    "    addchar(en2id,id2en,one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47028, 4770)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en2id), len(ch2id)\n",
    "# ch2id['<unk>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for english to chinese translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_x_in, dat_y_in,dat_y_out =[], [], []\n",
    "dat_x_len, dat_y_len = [], []\n",
    "unknown_en_id = en2id['<unk>']\n",
    "eos_en_id = en2id['<eos>']\n",
    "# english sentences to id array\n",
    "for en_sentence in enlines:\n",
    "    id_en_sentence = [ en2id.get(w, unknown_en_id) for w in en_sentence ]\n",
    "    id_en_sentence.append(eos_en_id) #mark end of sentence\n",
    "    dat_x_in.append(id_en_sentence)\n",
    "    dat_x_len.append(len(id_en_sentence))\n",
    "    \n",
    "# chinse sentences to id array\n",
    "start_ch_id = ch2id['<start>']\n",
    "end_ch_id = ch2id['<end>']\n",
    "unknown_ch_id = ch2id['<unk>']\n",
    "for ch_sentence in chlines:\n",
    "    id_ch_sentence = [ ch2id.get(w, unknown_ch_id) for w in ch_sentence ]\n",
    "    dat_y_in.append([start_ch_id] + id_ch_sentence)\n",
    "    dat_y_out.append(id_ch_sentence + [end_ch_id])\n",
    "    dat_y_len.append(len(id_ch_sentence) + 1)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['one', 'pair', 'of', 'elongated', 'canines', '.', '<eos>'],\n",
       " ['<start>', '一', '对', '延', '长', '犬', '齿', '。'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = 100\n",
    "[id2en[i] for i in dat_x_in[m]], [id2ch[i] for i in dat_y_in[m]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build input and lable\n",
    "data_x_in, data_y = [],[]\n",
    "\n",
    "for ch_sentence, en_sentence in zip(chlines, enlines):\n",
    "    ch_id_sen = [ch2id.get(i, unknown_ch_id) for i in ch_sentence]\n",
    "    data_x_in.append(ch_id_sen)\n",
    "    data_y.append(0)\n",
    "\n",
    "    en_id_sen = [en2id.get(i, unknown_en_id) for i in en_sentence]\n",
    "    data_x_in.append(en_id_sen)\n",
    "    data_y.append(1)\n",
    "\n",
    "data_x_in = tf.keras.preprocessing.sequence.pad_sequences(data_x_in, padding='post', value=en2id['<pad>'])\n",
    "data_y = np.asarray(data_y)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buil model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "embedding_size = 100\n",
    "vocabulary_size = len(en2id)\n",
    "num_units = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping: no known devices.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miche\\AppData\\Local\\Temp\\ipykernel_17308\\3615739391.py:21: UserWarning: `tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  encoder_cell = tfc.nn.rnn_cell.BasicLSTMCell(num_units)\n",
      "C:\\Users\\miche\\AppData\\Local\\Temp\\ipykernel_17308\\3615739391.py:25: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  model_logistic = tfc.layers.dense(encoder_states[0], 1)\n"
     ]
    }
   ],
   "source": [
    "tfc.reset_default_graph()\n",
    "tfc.disable_eager_execution()\n",
    "config = tfc.ConfigProto(log_device_placement =True, allow_soft_placement = True)\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tfc.Session(config = config)\n",
    "\n",
    "with tfc.device('/gpu:1'):\n",
    "    initializer =tfc.random_uniform_initializer(-0.08, 0.08)\n",
    "    tfc.get_variable_scope().set_initializer(initializer)\n",
    "\n",
    "    x = tfc.placeholder('int32', [None, None])\n",
    "    y = tfc.placeholder('int32', [None])\n",
    "    x_len = tfc.placeholder('int32', [None]) # control sequence lenth for RNN\n",
    "    learning_rate = tfc.placeholder(tfc.float32, shape=[])\n",
    "\n",
    "    # embedding\n",
    "    embedding_encoder = tfc.get_variable('embedding_encoder', [vocabulary_size, embedding_size], dtype=tfc.float32)\n",
    "    encode_embedding_inp = tfc.nn.embedding_lookup(embedding_encoder, x)\n",
    "\n",
    "    # build rnn cell\n",
    "    encoder_cell = tfc.nn.rnn_cell.BasicLSTMCell(num_units)\n",
    "    encoder_outputs, encoder_states = tfc.nn.dynamic_rnn(encoder_cell, encode_embedding_inp, sequence_length= x_len, time_major=False, dtype=tfc.float32)\n",
    "\n",
    "    # loss\n",
    "    model_logistic = tfc.layers.dense(encoder_states[0], 1)\n",
    "    model_prediction = tfc.nn.sigmoid(model_logistic)\n",
    "    loss = tfc.nn.sigmoid_cross_entropy_with_logits(labels=tfc.cast(y, tfc.float32), logits=tfc.reshape(model_logistic, (-1,)))\n",
    "    loss1 = tfc.reduce_mean(loss)\n",
    "    optimizer = tfc.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generate(data_x_in, data_y, batch_size = 1000):\n",
    "    for i in range(0, len(data_x_in), batch_size):\n",
    "        if i + batch_size < len(data_x_in):\n",
    "            yield data_x_in[i: i + batch_size], data_y[i: i + batch_size]\n",
    "        else:\n",
    "            yield data_x_in[i: len(data_x_in)], data_y[i: len(data_x_in)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = []\n",
    "begging_learning_rate = 0.1\n",
    "gen = data_generate(data_x_in, data_y, batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
