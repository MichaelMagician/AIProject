{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mushroom categorization with adaboost manual implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
    "from typing import Literal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  class cap-shape cap-surface cap-color bruises odor gill-attachment  \\\n",
      "0     p         x           s         n       t    p               f   \n",
      "1     e         x           s         y       t    a               f   \n",
      "\n",
      "  gill-spacing gill-size gill-color  ... stalk-surface-below-ring  \\\n",
      "0            c         n          k  ...                        s   \n",
      "1            c         b          k  ...                        s   \n",
      "\n",
      "  stalk-color-above-ring stalk-color-below-ring veil-type veil-color  \\\n",
      "0                      w                      w         p          w   \n",
      "1                      w                      w         p          w   \n",
      "\n",
      "  ring-number ring-type spore-print-color population habitat  \n",
      "0           o         p                 k          s       u  \n",
      "1           o         p                 n          n       g  \n",
      "\n",
      "[2 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('./data/mushrooms.csv')\n",
    "print(data.head(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['cap-shape_x', 'cap-shape_b', 'cap-shape_s', 'cap-shape_f',\n",
      "       'cap-shape_k', 'cap-shape_c', 'cap-surface_s', 'cap-surface_y',\n",
      "       'cap-surface_f', 'cap-surface_g',\n",
      "       ...\n",
      "       'population_v', 'population_y', 'population_c', 'habitat_u',\n",
      "       'habitat_g', 'habitat_m', 'habitat_d', 'habitat_p', 'habitat_w',\n",
      "       'habitat_l'],\n",
      "      dtype='object', length=117)\n",
      "   cap-shape_x  cap-shape_b  cap-shape_s  cap-shape_f  cap-shape_k  \\\n",
      "0         True        False        False        False        False   \n",
      "1         True        False        False        False        False   \n",
      "\n",
      "   cap-shape_c  cap-surface_s  cap-surface_y  cap-surface_f  cap-surface_g  \\\n",
      "0        False           True          False          False          False   \n",
      "1        False           True          False          False          False   \n",
      "\n",
      "   ...  population_v  population_y  population_c  habitat_u  habitat_g  \\\n",
      "0  ...         False         False         False       True      False   \n",
      "1  ...         False         False         False      False       True   \n",
      "\n",
      "   habitat_m  habitat_d  habitat_p  habitat_w  habitat_l  \n",
      "0      False      False      False      False      False  \n",
      "1      False      False      False      False      False  \n",
      "\n",
      "[2 rows x 117 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([False,  True])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_dummy_columns(data: pd.DataFrame, columns : list) -> pd.DataFrame :\n",
    "    # data = data.dropna()\n",
    "    for col in columns:        \n",
    "        new_column_suffix = data[col].apply(lambda x: str(x)).unique()\n",
    "        new_cols = [ col + '_' + suffix for suffix in new_column_suffix ]\n",
    "        new_data =pd.get_dummies(data[col], prefix=col)[new_cols]\n",
    "        data = pd.concat([data, new_data], axis=1) \n",
    "        del data[col]\n",
    "    return data\n",
    "\n",
    "data.fillna('missing', inplace=True)\n",
    "data_x = data.iloc[:,1:]\n",
    "X = generate_dummy_columns(data_x, data_x.columns)\n",
    "Y = data['class'].apply(lambda x: +1 if x == 'p' else 0)\n",
    "\n",
    "print(X.columns)\n",
    "print(X.head(2))\n",
    "X['cap-shape_k'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build decision tree with sample weights ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_error_count(data:pd.DataFrame, sample_weights:pd.DataFrame, label_column:str) -> float:\n",
    "    count_positive = (sample_weights[data[label_column] == 1]).sum()\n",
    "    return min(count_positive , len(data) - count_positive)\n",
    "# _get_best_features\n",
    "def get_best_feature(remaining_features: list, data: pd.DataFrame, sample_weights:pd.DataFrame, label_column: str) -> str:\n",
    "    return get_best_feature_by_weighted_error_rate(remaining_features, data, sample_weights, label_column)\n",
    "\n",
    "def get_best_feature_by_weighted_error_rate(remaining_features: list, data: pd.DataFrame, sample_weights:pd.DataFrame, label_column: str) -> str:\n",
    "    total_count = len(data)\n",
    "    min_error_rate = 2\n",
    "    best_feature = ''\n",
    "    for feature in remaining_features:\n",
    "        left_sample_weights = sample_weights[data[feature] == 0]\n",
    "        right_sample_weights = sample_weights[data[feature] == 1]\n",
    "        left_data = data[data[feature] == 0]\n",
    "        right_data = data[data[feature] == 1]\n",
    "        # print(f'feature:{feature}, left_data:{left_data.shape},left_sample_weights:{left_sample_weights.shape}, right_data:{right_data.shape}, right_sample_weights:{right_sample_weights.shape}')\n",
    "        weighted_error_count_0 = get_weighted_error_count(left_data, left_sample_weights, label_column) \n",
    "        weighted_error_count_1 = get_weighted_error_count(right_data, right_sample_weights, label_column) \n",
    "        \n",
    "        error_rate = (weighted_error_count_0 + weighted_error_count_1) * 1.0 / sum(sample_weights)\n",
    "        if error_rate < min_error_rate:\n",
    "            min_error_rate = error_rate\n",
    "            best_feature = feature\n",
    "    return best_feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit()\n",
    "# predict()\n",
    "    # dfs visit from the root feature and use the leaf node for prediction \n",
    "# _create_tree()\n",
    "    # termniation. max_depth, no more split or good enough, reaching leaf\n",
    "    # find the best feature\n",
    "    # split and build left/right tree\n",
    "\n",
    "\n",
    "class TreeNode:\n",
    "    def __init__(self, feature_name: str) -> None:        \n",
    "        self.feature_name = feature_name\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.prediction = 0\n",
    "        self.is_leaf = False\n",
    "\n",
    "class MyWeightedDecisionTreeClassifier:\n",
    "    def __init__(self, min_error_rate =0.3, max_depth = 4) -> None:\n",
    "        self.min_error_rate = min_error_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.label_column = \"\"\n",
    "    \n",
    "    def fit(self, X : pd.DataFrame, Y : pd.Series, sample_weights: pd.Series):\n",
    "        data = pd.concat([X, Y], axis=1)\n",
    "        self.label_column = Y.name\n",
    "        self.root = self._create_tree(data, sample_weights, X.columns, 0 )      \n",
    "        print('finish fitting MyWeightedDecisionTreeClassifier')  \n",
    "    \n",
    "    def predict(self, X: pd.DataFrame):\n",
    "        return X.apply(lambda d: self._predict_single(d, self.root), axis=1)\n",
    "\n",
    "    def _predict_single(self, row_data, node=None):\n",
    "        if node == None:\n",
    "            node = self.root\n",
    "        if node.is_leaf:\n",
    "            return node.prediction\n",
    "        \n",
    "        if row_data[node.feature_name] == 0:\n",
    "            return self._predict_single(row_data, node.left)\n",
    "        else:\n",
    "            return self._predict_single(row_data, node.right)\n",
    "    \n",
    "    def _create_tree(self, data : pd.DataFrame, sample_weights:pd.Series, remaining_features: list, depth: int) -> TreeNode:\n",
    "        if depth == self.max_depth or len(remaining_features) == 0:\n",
    "            # print(f'terminated at depth {depth}')\n",
    "            return self._create_leaf(data[self.label_column], '')        \n",
    "        \n",
    "        # current node\n",
    "        best_feature = get_best_feature(remaining_features, data, sample_weights, self.label_column)\n",
    "        node = TreeNode(best_feature)\n",
    "        # print(f'split on feature {best_feature}' )        \n",
    "        # build left/right node\n",
    "        left_split_data = data[data[best_feature] == 0]\n",
    "        right_split_data = data[data[best_feature] == 1]\n",
    "        # print(f'split on feature : {best_feature}, {len(left_split_data)}, {len(right_split_data)}'  )\n",
    "\n",
    "        if(len(left_split_data) == len(data)):\n",
    "            # print('perfect left split')\n",
    "            return self._create_leaf(left_split_data[self.label_column] , best_feature)\n",
    "        elif(len(right_split_data) == len(data)):\n",
    "            # print('perfect right split')\n",
    "            return self._create_leaf(right_split_data[self.label_column] , best_feature)\n",
    "        \n",
    "        new_remaining_features = remaining_features.drop(best_feature)\n",
    "        left_sample_weights = sample_weights[data[best_feature] == 0]\n",
    "        right_sample_weights = sample_weights[data[best_feature] == 1]\n",
    "\n",
    "        left_node = self._create_tree(left_split_data, left_sample_weights, new_remaining_features, depth + 1)\n",
    "        right_node = self._create_tree(right_split_data, right_sample_weights, new_remaining_features, depth + 1)\n",
    "        \n",
    "        node.left = left_node\n",
    "        node.right = right_node\n",
    "        \n",
    "        return node        \n",
    "        \n",
    "    def _create_leaf(self, Y: pd.Series, feature_name: str = ''):\n",
    "        positive_count = (Y == 1).sum()\n",
    "        prediction =  1 if positive_count/len(Y) >= 0.5 else -1\n",
    "        node = TreeNode(feature_name)\n",
    "        node.prediction = prediction\n",
    "        node.is_leaf = True         \n",
    "        return node\n",
    "    \n",
    "    def count_leaves(self):\n",
    "        return self.count_leaves_helper(self.root)\n",
    "    \n",
    "    def count_leaves_helper(self, tree):\n",
    "        if tree.is_leaf:\n",
    "            return 1\n",
    "        return self.count_leaves_helper(tree.left) + self.count_leaves_helper(tree.right)\n",
    "\n",
    "    def score(self, test_x, test_y):\n",
    "        pred = self.predict(test_x)\n",
    "        return accuracy_score(test_y, pred)\n",
    "\n",
    "# model =  MyWeightedDecisionTreeClassifier(max_depth=10, min_error_rate=1e-8)\n",
    "# sample_weights = np.ones(len(X))\n",
    "# sample_weights = sample_weights / len(X)\n",
    "# model.fit(X, Y, sample_weights) \n",
    "# print(model.score(X,Y))     \n",
    "# # print(model.count_leaves())   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ada boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish fitting MyWeightedDecisionTreeClassifier\n",
      "sample weights sum:  1.0\n",
      "finish fitting MyWeightedDecisionTreeClassifier\n",
      "sample weights sum:  1.0\n",
      "finish fitting MyWeightedDecisionTreeClassifier\n",
      "sample weights sum:  1.0\n",
      "finish fitting MyWeightedDecisionTreeClassifier\n",
      "sample weights sum:  0.9999999999999999\n",
      "finish fitting MyWeightedDecisionTreeClassifier\n",
      "sample weights sum:  0.9999999999999999\n",
      "finish fitting MyWeightedDecisionTreeClassifier\n",
      "sample weights sum:  0.9999999999999999\n",
      "finish fitting MyWeightedDecisionTreeClassifier\n",
      "sample weights sum:  1.0\n",
      "finish fitting MyWeightedDecisionTreeClassifier\n",
      "sample weights sum:  1.0\n",
      "finish fitting MyWeightedDecisionTreeClassifier\n",
      "sample weights sum:  0.9999999999999999\n",
      "finish fitting MyWeightedDecisionTreeClassifier\n",
      "sample weights sum:  0.9999999999999998\n",
      "finish fitting MyWeightedDecisionTreeClassifier\n",
      "sample weights sum:  1.0\n",
      "finish fitting MyWeightedDecisionTreeClassifier\n",
      "sample weights sum:  1.0\n",
      "finish fitting MyWeightedDecisionTreeClassifier\n",
      "sample weights sum:  1.0000000000000002\n",
      "finish fitting MyWeightedDecisionTreeClassifier\n",
      "sample weights sum:  1.0\n",
      "finish fitting MyWeightedDecisionTreeClassifier\n",
      "sample weights sum:  1.0\n",
      "finish fitting MyWeightedDecisionTreeClassifier\n",
      "sample weights sum:  0.9999999999999999\n",
      "finish fitting MyWeightedDecisionTreeClassifier\n",
      "sample weights sum:  1.0\n",
      "finish fitting MyWeightedDecisionTreeClassifier\n",
      "sample weights sum:  0.9999999999999999\n",
      "finish fitting MyWeightedDecisionTreeClassifier\n",
      "sample weights sum:  1.0\n",
      "finish fitting MyWeightedDecisionTreeClassifier\n",
      "sample weights sum:  1.0\n",
      "finish fitting MyWeightedDecisionTreeClassifier\n",
      "sample weights sum:  1.0\n",
      "finish fitting MyWeightedDecisionTreeClassifier\n",
      "sample weights sum:  0.9999999999999998\n",
      "finish fitting MyWeightedDecisionTreeClassifier\n",
      "sample weights sum:  1.0\n",
      "finish fitting MyWeightedDecisionTreeClassifier\n",
      "sample weights sum:  1.0\n",
      "finish fitting MyWeightedDecisionTreeClassifier\n",
      "sample weights sum:  1.0\n",
      "finish fitting MyWeightedDecisionTreeClassifier\n",
      "sample weights sum:  0.9999999999999999\n",
      "finish fitting MyWeightedDecisionTreeClassifier\n",
      "sample weights sum:  1.0\n",
      "finish fitting MyWeightedDecisionTreeClassifier\n",
      "sample weights sum:  1.0000000000000002\n",
      "finish fitting MyWeightedDecisionTreeClassifier\n",
      "sample weights sum:  0.9999999999999998\n",
      "finish fitting MyWeightedDecisionTreeClassifier\n",
      "sample weights sum:  1.0\n",
      "my_adboost score: 0.2838503200393895\n"
     ]
    }
   ],
   "source": [
    "# fit\n",
    "    # pick model number\n",
    "        # initial sample\n",
    "        # sample weight\n",
    "        # pick a feature\n",
    "        # train current model\n",
    "        # calculate model weight\n",
    "        # update sample weight weight\n",
    "\n",
    "# predict \n",
    "    # get the weighted predictions of all the models\n",
    "from sklearn.base import BaseEstimator\n",
    "class MyAdaBoostClassifier(BaseEstimator):\n",
    "    def __init__(self, N = 10):\n",
    "        self.N = N\n",
    "        self.models = []\n",
    "        self.model_weights = []\n",
    "\n",
    "    def fit(self, X, y):        \n",
    "        sample_weights = np.ones(len(X)) / len(X)\n",
    "        \n",
    "        for i in range(self.N):\n",
    "            model = MyWeightedDecisionTreeClassifier(max_depth=2)\n",
    "            model.fit(X, y, sample_weights)\n",
    "            y_pred = model.predict(X)\n",
    "            \n",
    "            # calculate new model weight\n",
    "            weighted_error_rate = sample_weights.dot(y_pred != y)  \n",
    "            if weighted_error_rate > 1:\n",
    "                weighted_error_rate = 9e-23\n",
    "\n",
    "            model_weight = 0.5 * np.log((1- weighted_error_rate)/ weighted_error_rate)\n",
    "            \n",
    "        \n",
    "            # new normalized sample weights\n",
    "            new_sample_weights = sample_weights* np.exp(-model_weight * y_pred * y )\n",
    "            sample_weights = new_sample_weights/new_sample_weights.sum() \n",
    "            \n",
    "            print('sample weights sum: ' , sample_weights.sum())\n",
    "\n",
    "            self.models.append(model)\n",
    "            self.model_weights.append(model_weight)  \n",
    "        \n",
    "\n",
    "    def predict(self, X):\n",
    "        result = np.zeros(len(X))\n",
    "        for i in range(self.N):\n",
    "            result += self.model_weights[i] * self.models[i].predict(X)\n",
    "        return np.sign(result)\n",
    "\n",
    "    def score(self, X, y):        \n",
    "        y_pred = self.predict(X)\n",
    "        return accuracy_score(y, y_pred)\n",
    "\n",
    "my_adboost = MyAdaBoostClassifier(N=10)\n",
    "# my_adboost.fit(X, Y)\n",
    "# accuracy_score(Y, my_adboost.predict(X))\n",
    "score4 = cross_val_score(my_adboost, X, Y, cv=3)\n",
    "print('my_adboost score:', score4.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predit on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df = pd.DataFrame()\n",
    "# df.to_csv('submission.csv',index = False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
